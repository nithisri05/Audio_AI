{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Speech-to-Text Robustness Analysis Using Whisper\n",
        "\n",
        "## An Experimental Study on Audio Format and Noise Variations\n",
        "\n",
        "This notebook explores how OpenAI's Whisper model performs under\n",
        "different audio formats and distortion conditions.\n",
        "\n",
        "We conduct controlled experiments using:\n",
        "\n",
        "- Real human voice recording\n",
        "- Multiple audio formats (WAV, MP3, low bitrate)\n",
        "- Telephone-quality audio\n",
        "- Background noise (moderate and heavy)\n",
        "- Echo effects\n",
        "- Word Error Rate (WER) analysis\n",
        "- Inference time comparison\n",
        "\n",
        "The objective is to analyze how signal distortion affects\n",
        "automatic speech recognition performance.\n"
      ],
      "metadata": {
        "id": "CencAGrMFgRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective\n",
        "\n",
        "The goal of this experiment is to:\n",
        "\n",
        "1. Record real human speech.\n",
        "2. Transcribe it using Whisper.\n",
        "3. Modify the audio under different conditions.\n",
        "4. Compare transcription accuracy.\n",
        "5. Measure Word Error Rate (WER).\n",
        "6. Evaluate model robustness.\n",
        "\n",
        "This study demonstrates how audio degradation impacts\n",
        "speech recognition systems.\n"
      ],
      "metadata": {
        "id": "g5U2RICjFybo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Recording Real Voice Input\n",
        "\n",
        "In this section, we record a 2-line spoken sentence using the browser microphone.\n",
        "\n",
        "Sentence spoken:\n",
        "\n",
        "> \"Machine learning is transforming healthcare.  \n",
        "> Early diagnosis improves patient survival rates.\"\n",
        "\n",
        "This serves as the baseline audio sample for all further experiments.\n"
      ],
      "metadata": {
        "id": "yt3q9TMtF2RK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4IWPc6u85l6"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "\n",
        "RECORD_SECONDS = 10\n",
        "\n",
        "print(\"Recording... Speak now.\")\n",
        "\n",
        "js = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = async () => {\n",
        "  const stream = await navigator.mediaDevices.getUserMedia({audio:true})\n",
        "  const recorder = new MediaRecorder(stream)\n",
        "  let chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(%d * 1000)\n",
        "  recorder.stop()\n",
        "  await sleep(1000)\n",
        "  const blob = new Blob(chunks)\n",
        "  const text = await b2text(blob)\n",
        "  return text\n",
        "}\n",
        "record()\n",
        "\"\"\" % RECORD_SECONDS\n",
        "\n",
        "audio_data = output.eval_js(js)\n",
        "binary = b64decode(audio_data.split(',')[1])\n",
        "\n",
        "with open('my_voice.webm', 'wb') as f:\n",
        "    f.write(binary)\n",
        "\n",
        "print(\"Saved as my_voice.webm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Converting Recorded Audio to WAV Format\n",
        "\n",
        "The recorded file is saved as `.webm`.\n",
        "\n",
        "Whisper performs best with WAV format, so we convert the\n",
        "audio file into `.wav` using FFmpeg.\n",
        "\n",
        "This ensures compatibility and stable preprocessing.\n"
      ],
      "metadata": {
        "id": "kpWduqrAF6oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update && apt install -y ffmpeg\n"
      ],
      "metadata": {
        "id": "gwg3rm1b95By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i my_voice.webm my_voice.wav\n"
      ],
      "metadata": {
        "id": "spU4bWPv99if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Installing and Testing Whisper (Base Model)\n",
        "\n",
        "We install OpenAI Whisper and test the transcription\n",
        "using the Base model.\n",
        "\n",
        "The Base model has moderate parameter size and\n",
        "serves as an initial evaluation baseline.\n"
      ],
      "metadata": {
        "id": "PMOSVLO3F-9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai-whisper\n",
        "!apt update && apt install -y ffmpeg\n"
      ],
      "metadata": {
        "id": "fgX57Lfs-H1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "result = model.transcribe(\n",
        "    \"my_voice.wav\",\n",
        "    language=\"en\",\n",
        "    temperature=0,\n",
        "    fp16=False\n",
        ")\n",
        "\n",
        "print(\"BASE OUTPUT:\")\n",
        "print(result[\"text\"])\n"
      ],
      "metadata": {
        "id": "08pHhYdg9_jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 4: Testing with a Larger Model (Medium)\n",
        "\n",
        "We now load the Medium model, which has significantly\n",
        "more parameters and better language modeling capability.\n",
        "\n",
        "This helps us observe how model size affects\n",
        "transcription accuracy.\n"
      ],
      "metadata": {
        "id": "Cob1Sqc4GCIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"medium\")\n",
        "\n",
        "result = model.transcribe(\n",
        "    \"my_voice.wav\",\n",
        "    language=\"en\",\n",
        "    temperature=0,\n",
        "    fp16=False\n",
        ")\n",
        "\n",
        "print(\"MEDIUM MODEL OUTPUT:\")\n",
        "print(result[\"text\"])\n"
      ],
      "metadata": {
        "id": "Ex4fraPc-h2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 5: Speed Variation (Fast Speech)\n",
        "\n",
        "We artificially increase the playback speed of the audio\n",
        "using the `atempo` filter.\n",
        "\n",
        "This tests Whisper’s ability to handle rapid speech\n",
        "and temporal distortion.\n"
      ],
      "metadata": {
        "id": "uxr06nVIGGG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i my_voice.wav -filter:a \"atempo=1.8\" fast_voice.wav\n"
      ],
      "metadata": {
        "id": "_2L-wT3J_FCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(\n",
        "    \"fast_voice.wav\",\n",
        "    language=\"en\",\n",
        "    temperature=0,\n",
        "    fp16=False\n",
        ")\n",
        "\n",
        "print(\"FAST VERSION OUTPUT:\")\n",
        "print(result[\"text\"])\n"
      ],
      "metadata": {
        "id": "3AJseWHo_Hwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Audio Format Variations\n",
        "\n",
        "We now test how different audio formats affect transcription:\n",
        "\n",
        "- Standard MP3\n",
        "- Low bitrate MP3 (high compression)\n",
        "\n",
        "This helps analyze whether file compression impacts\n",
        "speech recognition performance.\n"
      ],
      "metadata": {
        "id": "WyVn49b-GOQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(\n",
        "    \"my_voice.wav\",\n",
        "    language=\"en\",\n",
        "    temperature=0,\n",
        "    fp16=False\n",
        ")\n",
        "\n",
        "print(\"BASELINE:\")\n",
        "print(result[\"text\"])\n"
      ],
      "metadata": {
        "id": "-QVskkcYCReN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i my_voice.wav my_voice.mp3\n"
      ],
      "metadata": {
        "id": "kMBP3guMCb9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(\n",
        "    \"my_voice.mp3\",\n",
        "    language=\"en\",\n",
        "    temperature=0,\n",
        "    fp16=False\n",
        ")\n",
        "\n",
        "print(\"MP3 OUTPUT:\")\n",
        "print(result[\"text\"])\n"
      ],
      "metadata": {
        "id": "_sri6Jn9CeKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i my_voice.wav -b:a 32k low_bitrate.mp3\n"
      ],
      "metadata": {
        "id": "JcczjDR5Co9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(\n",
        "    \"low_bitrate.mp3\",\n",
        "    language=\"en\",\n",
        "    temperature=0,\n",
        "    fp16=False\n",
        ")\n",
        "\n",
        "print(\"LOW BITRATE OUTPUT:\")\n",
        "print(result[\"text\"])\n"
      ],
      "metadata": {
        "id": "2eBwx1P_Crac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 7: Telephone-Quality Audio Simulation\n",
        "\n",
        "We simulate a phone call environment by:\n",
        "\n",
        "- Reducing sampling rate to 8kHz\n",
        "- Applying bandpass filtering (300Hz – 3400Hz)\n",
        "\n",
        "This mimics real-world telecommunication audio\n",
        "and tests model robustness under limited frequency range.\n"
      ],
      "metadata": {
        "id": "WmzKPxZGGSt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i my_voice.wav -ar 8000 -af \"highpass=f=300, lowpass=f=3400\" telephone.wav\n"
      ],
      "metadata": {
        "id": "1pm_NYyZC0zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(\n",
        "    \"telephone.wav\",\n",
        "    language=\"en\",\n",
        "    temperature=0,\n",
        "    fp16=False\n",
        ")\n",
        "\n",
        "print(\"TELEPHONE OUTPUT:\")\n",
        "print(result[\"text\"])\n"
      ],
      "metadata": {
        "id": "ZGtSIgSIC3BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 8: Background Noise Simulation\n",
        "\n",
        "We introduce controlled white noise at two levels:\n",
        "\n",
        "1. Moderate Noise\n",
        "2. Heavy Noise\n",
        "\n",
        "This evaluates how phoneme distortion affects\n",
        "transcription accuracy.\n"
      ],
      "metadata": {
        "id": "Sui7VW-hGYmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i my_voice.wav -filter_complex \"anoisesrc=color=white:amplitude=0.2 [noise]; [0:a][noise] amix=inputs=2:duration=shortest\" moderate_noise.wav\n"
      ],
      "metadata": {
        "id": "lKv7w-KfDAlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i my_voice.wav -filter_complex \"anoisesrc=color=white:amplitude=0.6 [noise]; [0:a][noise] amix=inputs=2:duration=shortest\" heavy_noise.wav\n"
      ],
      "metadata": {
        "id": "vPBYLI-tDD-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Echo Effect Simulation\n",
        "\n",
        "We apply an echo filter to simulate room reverberation.\n",
        "\n",
        "Echo introduces temporal overlap in audio signals\n",
        "and challenges the model’s temporal understanding.\n"
      ],
      "metadata": {
        "id": "jDsj30eeGbHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i my_voice.wav -filter:a \"aecho=0.8:0.9:1000:0.3\" echo.wav\n"
      ],
      "metadata": {
        "id": "AQHfqpfPDGBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer\n"
      ],
      "metadata": {
        "id": "wwV9jA5HDKOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 10: Word Error Rate (WER) Calculation\n",
        "\n",
        "To quantitatively evaluate performance,\n",
        "we calculate Word Error Rate (WER).\n",
        "\n",
        "WER measures the difference between:\n",
        "\n",
        "- Reference sentence (ground truth)\n",
        "- Transcribed output\n",
        "\n",
        "WER = (Substitutions + Insertions + Deletions) / Total Words\n",
        "\n",
        "Lower WER indicates better accuracy.\n"
      ],
      "metadata": {
        "id": "xsiQTFgvGecQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference = \"Machine learning is transforming healthcare. Early diagnosis improves patient survival rates.\"\n"
      ],
      "metadata": {
        "id": "wT4Se9P8DnFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from jiwer import wer\n",
        "\n",
        "files = [\n",
        "    \"my_voice.wav\",\n",
        "    \"my_voice.mp3\",\n",
        "    \"low_bitrate.mp3\",\n",
        "    \"telephone.wav\",\n",
        "    \"moderate_noise.wav\",\n",
        "    \"heavy_noise.wav\",\n",
        "    \"echo.wav\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for f in files:\n",
        "    start = time.time()\n",
        "\n",
        "    result = model.transcribe(\n",
        "        f,\n",
        "        language=\"en\",\n",
        "        temperature=0,\n",
        "        fp16=False\n",
        "    )\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    transcription = result[\"text\"]\n",
        "    error = wer(reference.lower(), transcription.lower())\n",
        "    inference_time = round(end - start, 2)\n",
        "\n",
        "    results.append((f, transcription, error, inference_time))\n",
        "\n",
        "for r in results:\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"File:\", r[0])\n",
        "    print(\"Transcription:\", r[1])\n",
        "    print(\"WER:\", r[2])\n",
        "    print(\"Inference Time (s):\", r[3])\n"
      ],
      "metadata": {
        "id": "IG3Lp3LIDl7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 11: Performance Visualization\n",
        "\n",
        "We organize the results into a structured DataFrame\n",
        "and visualize WER across different audio conditions.\n",
        "\n",
        "This provides a clear comparison of model robustness\n",
        "under varying signal distortions.\n"
      ],
      "metadata": {
        "id": "WWlgJcc-Gj2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results, columns=[\"File\", \"Transcription\", \"WER\", \"Inference Time (s)\"])\n",
        "df\n"
      ],
      "metadata": {
        "id": "VjkwYtZEDqXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(df[\"File\"], df[\"WER\"])\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel(\"Word Error Rate\")\n",
        "plt.title(\"WER Across Audio Conditions\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3sbhzlJ8EtzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observations\n",
        "\n",
        "1. The Medium model significantly outperformed the Base model.\n",
        "2. Simple format changes (WAV vs MP3) had minimal impact.\n",
        "3. Low bitrate compression slightly affected clarity.\n",
        "4. Telephone-quality audio reduced frequency information but remained understandable.\n",
        "5. Moderate noise caused minor degradation.\n",
        "6. Heavy noise significantly increased WER.\n",
        "7. Echo introduced temporal confusion but did not completely fail transcription.\n",
        "\n",
        "Key Insight:\n",
        "Audio signal distortion impacts transcription more than file format differences.\n"
      ],
      "metadata": {
        "id": "rUUXeiVvGn-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This experiment demonstrates that:\n",
        "\n",
        "- Whisper is highly robust to format variations.\n",
        "- Model size plays a crucial role in handling unclear audio.\n",
        "- Severe background noise significantly impacts performance.\n",
        "- Band-limited audio (telephone quality) still performs reasonably well.\n",
        "- Word Error Rate (WER) is an effective quantitative metric.\n",
        "\n",
        "Overall, Whisper shows strong real-world robustness\n",
        "but remains sensitive to extreme acoustic distortions.\n"
      ],
      "metadata": {
        "id": "r_g7RlQ-Gpfl"
      }
    }
  ]
}