{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook introduces the fundamental concepts of audio processing that are required before building **Speech Recognition (ASR)** or **Voice AI systems**.\n",
        "\n",
        "The goal of this notebook is to understand how raw audio is represented, visualized, and transformed into meaningful features such as spectrograms and MFCCs, which are the backbone of traditional and modern ASR systems."
      ],
      "metadata": {
        "id": "6R52PWGfB4TV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What this notebook contains\n",
        "\n",
        "This notebook covers the following concepts step by step:\n",
        "\n",
        "* Loading an audio file\n",
        "\n",
        "* Visualizing the raw audio waveform\n",
        "\n",
        "* Converting audio from time domain to frequency domain\n",
        "\n",
        "* Generating spectrograms and mel-spectrograms\n",
        "\n",
        "* Extracting MFCC features\n",
        "\n",
        "These steps build the complete audio processing pipeline used in speech systems."
      ],
      "metadata": {
        "id": "qRkEbh5YCAFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa matplotlib soundfile -q"
      ],
      "metadata": {
        "id": "9kertZ9M8BkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load an Audio File"
      ],
      "metadata": {
        "id": "2NYp7gc-8HEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load audio file\n",
        "file_path = \"/content/jewellery_audio.wav\" # built-in sample from librosa\n",
        "y, sr = librosa.load(file_path, sr=None)  # y = waveform, sr = sample rate\n",
        "\n",
        "print(f\"Audio loaded: {'/content/jewellery_audio.wav'}\")\n",
        "print(f\"Duration: {len(y)/sr:.2f} seconds, Sample Rate: {sr}\")"
      ],
      "metadata": {
        "id": "dwKSd2fK8Ipg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Visualize Raw Waveform"
      ],
      "metadata": {
        "id": "vSz19MCR_eGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 4))\n",
        "librosa.display.waveshow(y, sr=sr)\n",
        "plt.title(\"Raw Waveform\")\n",
        "plt.xlabel(\"Time (s)\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X5EaM4M1_dl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Generate a Spectrogram"
      ],
      "metadata": {
        "id": "0IppEccl_kb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Short-Time Fourier Transform (STFT)\n",
        "D = np.abs(librosa.stft(y))\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max),\n",
        "                         sr=sr, x_axis='time', y_axis='log')\n",
        "plt.colorbar(format=\"%+2.0f dB\")\n",
        "plt.title(\"Spectrogram (Log-Frequency Scale)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EIVBmwXD_lLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Generate a Mel-Spectrogram"
      ],
      "metadata": {
        "id": "f5vNhq2K_pKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mel-scaled spectrogram\n",
        "S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128,\n",
        "                                   fmax=8000)\n",
        "S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "librosa.display.specshow(S_dB, sr=sr, x_axis='time',\n",
        "                         y_axis='mel', fmax=8000)\n",
        "plt.colorbar(format=\"%+2.0f dB\")\n",
        "plt.title(\"Mel-Spectrogram\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OxuLiCzb_qEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Extract MFCC Features"
      ],
      "metadata": {
        "id": "e5fb9yFo_wGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Compute MFCCs\n",
        "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "librosa.display.specshow(mfccs, x_axis='time')\n",
        "plt.colorbar()\n",
        "plt.title(\"MFCC (Mel-Frequency Cepstral Coefficients)\")\n",
        "plt.ylabel(\"MFCC Coefficients\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.show()\n",
        "\n",
        "print(\"MFCC shape:\", mfccs.shape)"
      ],
      "metadata": {
        "id": "4bpMiYAW_xA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Observation â€“ Audio Basics & Visualization\n",
        "\n",
        "This notebook demonstrates how raw speech audio is transformed into meaningful visual and numerical representations used in speech processing. It shows the progression from time-domain waveforms to frequency-domain representations such as spectrograms, mel-spectrograms, and MFCCs, highlighting how each representation captures different characteristics of speech. Through visualization, it becomes clear that raw waveforms alone are insufficient for speech understanding, while frequency-based and Mel-scaled features effectively represent phonetic and perceptual information. Overall, the notebook establishes the foundational audio concepts required for both legacy and modern speech recognition systems."
      ],
      "metadata": {
        "id": "b90u-u1ICqb_"
      }
    }
  ]
}