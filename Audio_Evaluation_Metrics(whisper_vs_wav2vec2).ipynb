{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ASR Model Evaluation on Real-World Multilingual Audio\n",
        "\n",
        "## Objective\n",
        "This notebook evaluates Automatic Speech Recognition (ASR) performance using real-world multilingual audio.\n",
        "\n",
        "Two models are compared:\n",
        "- Whisper-1 (OpenAI)\n",
        "- Wav2Vec2 (Facebook)\n",
        "\n",
        "Evaluation Metrics:\n",
        "- Word Error Rate (WER)\n",
        "- Character Error Rate (CER)\n",
        "- Real-Time Factor (RTF)\n",
        "- Inverse RTF\n"
      ],
      "metadata": {
        "id": "5REBON6NGeG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Dataset Description\n",
        "\n",
        "The input audio satisfies all assignment requirements:\n",
        "\n",
        "- Duration: Greater than 8 minutes\n",
        "- Speakers: Minimum 5 (human + synthetic)\n",
        "- Accents: Indian, American, British\n",
        "- Languages: English, Tamil, Hindi\n",
        "- Noise: Fan noise and conversational variation\n",
        "- Format: WAV, 16 kHz, mono\n",
        "\n",
        "Audio segments include:\n",
        "1. Indian English speech\n",
        "2. American accent speech\n",
        "3. British accent speech with noise\n",
        "4. Tamil speech\n",
        "5. Hindi speech\n",
        "6. Synthetic English voice\n",
        "7. Informal conversational speech\n"
      ],
      "metadata": {
        "id": "DIiyV6nRGjcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Google Drive Connection"
      ],
      "metadata": {
        "id": "mMcscKlgHuwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "COgXB88x-Aj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "project_path = \"/content/drive/MyDrive/ASR_Evaluation_Project\"\n",
        "\n",
        "os.makedirs(project_path, exist_ok=True)\n",
        "\n",
        "print(\"Project folder created at:\", project_path)\n"
      ],
      "metadata": {
        "id": "C-v_q9ED-IOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install soundfile\n"
      ],
      "metadata": {
        "id": "9g9c6B_X_jMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ASR_Evaluation_Project\n"
      ],
      "metadata": {
        "id": "qk3EH72l-KJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n"
      ],
      "metadata": {
        "id": "RqwuPH6n-XNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/symblai/speech-recognition-evaluation.git\n"
      ],
      "metadata": {
        "id": "5Jo9H7QC-ZMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "id": "7NWOHU_l-bpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Audio Recording Setup\n",
        "\n",
        "This section defines a custom audio recording function using Google Colab's browser microphone access.\n",
        "\n",
        "The function:\n",
        "- Requests microphone permission\n",
        "- Records audio for a specified duration\n",
        "- Saves the recording as `.webm`\n",
        "- Converts it to `.wav` format using FFmpeg\n",
        "\n",
        "This approach enables collection of real-world speech directly within Colab.\n"
      ],
      "metadata": {
        "id": "Az6dQqjvH_33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "import time\n",
        "\n",
        "def record_audio(filename, record_seconds=60):\n",
        "    print(\"Get ready...\")\n",
        "    time.sleep(2)\n",
        "    print(\"Recording will start in 3 seconds...\")\n",
        "    time.sleep(3)\n",
        "    print(\"ЁЯОЩя╕П Recording NOW. Speak clearly.\")\n",
        "\n",
        "    js = f\"\"\"\n",
        "    async function recordAudio() {{\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({{audio: true}});\n",
        "      const recorder = new MediaRecorder(stream);\n",
        "      let chunks = [];\n",
        "\n",
        "      recorder.ondataavailable = e => chunks.push(e.data);\n",
        "\n",
        "      recorder.start();\n",
        "      await new Promise(resolve => setTimeout(resolve, {record_seconds} * 1000));\n",
        "      recorder.stop();\n",
        "\n",
        "      await new Promise(resolve => recorder.onstop = resolve);\n",
        "\n",
        "      const blob = new Blob(chunks);\n",
        "      const reader = new FileReader();\n",
        "      reader.readAsDataURL(blob);\n",
        "\n",
        "      return new Promise(resolve => {{\n",
        "        reader.onloadend = () => resolve(reader.result);\n",
        "      }});\n",
        "    }}\n",
        "\n",
        "    recordAudio();\n",
        "    \"\"\"\n",
        "\n",
        "    audio_data = output.eval_js(js)\n",
        "    binary = b64decode(audio_data.split(',')[1])\n",
        "\n",
        "    # Save as webm first\n",
        "    webm_filename = filename.replace(\".wav\", \".webm\")\n",
        "    with open(webm_filename, \"wb\") as f:\n",
        "        f.write(binary)\n",
        "\n",
        "    print(f\"Saved raw recording as {webm_filename}\")\n",
        "\n",
        "    # Convert to WAV using ffmpeg\n",
        "    !ffmpeg -loglevel quiet -i \"{webm_filename}\" \"{filename}\"\n",
        "\n",
        "    print(f\"тЬЕ Converted and saved as {filename}\")\n"
      ],
      "metadata": {
        "id": "vLWUa6xs-mGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segment 1 тАУ Indian English Accent\n",
        "\n",
        "This segment captures natural Indian English speech in a relatively clean environment.\n",
        "\n",
        "Purpose:\n",
        "- Evaluate accent robustness\n",
        "- Establish baseline English performance\n"
      ],
      "metadata": {
        "id": "wwaGvmGJIFGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "record_audio(\"segment1.wav\", record_seconds=120)\n"
      ],
      "metadata": {
        "id": "EKKdGJ1o_zaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv segment1.webm /content/drive/MyDrive/ASR_Evaluation_Project/\n",
        "!mv segment1.wav /content/drive/MyDrive/ASR_Evaluation_Project/\n"
      ],
      "metadata": {
        "id": "UJlmE803CfCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segment 2 тАУ American Accent (Light Background Noise)\n",
        "\n",
        "This segment simulates American-style pronunciation with slight environmental noise.\n",
        "\n",
        "Purpose:\n",
        "- Test accent variation handling\n",
        "- Evaluate noise sensitivity\n"
      ],
      "metadata": {
        "id": "6B5sJYNlIHJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "record_audio(\"segment2.wav\", record_seconds=90)\n"
      ],
      "metadata": {
        "id": "W8Cem3saCxgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segment 3 тАУ British Accent (Noticeable Background Noise)\n",
        "\n",
        "This segment includes British pronunciation and more noticeable background noise.\n",
        "\n",
        "Purpose:\n",
        "- Evaluate robustness to pronunciation shifts\n",
        "- Analyze performance under environmental disturbances\n"
      ],
      "metadata": {
        "id": "6t2mUsK_IJim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "record_audio(\"segment3.wav\", record_seconds=90)\n"
      ],
      "metadata": {
        "id": "d_-k846g7ual"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segment 4 тАУ Tamil Speech\n",
        "\n",
        "This segment includes natural Tamil speech recorded by the speaker.\n",
        "\n",
        "Purpose:\n",
        "- Evaluate multilingual capability\n",
        "- Test non-English transcription performance\n"
      ],
      "metadata": {
        "id": "5bLqf_ILILDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "record_audio(\"segment4.wav\", record_seconds=90)\n"
      ],
      "metadata": {
        "id": "VCj1w9ji8qd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segment 5 тАУ Hindi Speech\n",
        "\n",
        "This segment includes Hindi language content.\n",
        "\n",
        "Purpose:\n",
        "- Evaluate multilingual robustness\n",
        "- Test model handling of Devanagari script\n"
      ],
      "metadata": {
        "id": "ObGEaMvBIMtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gtts\n"
      ],
      "metadata": {
        "id": "RCfyBuT_-K1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "\n",
        "hindi_text = \"\"\"\n",
        "рдирдорд╕реНрддреЗ, рдпрд╣ рд░рд┐рдХреЙрд░реНрдбрд┐рдВрдЧ рд╣рд┐рдВрджреА рднрд╛рд╖рд╛ рдореЗрдВ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд╡рд╛рдХреН рдкрд╣рдЪрд╛рди рдкреНрд░рдгрд╛рд▓реА рдХреЗ рдореВрд▓реНрдпрд╛рдВрдХрди рдХреЗ рд▓рд┐рдП рдмрдирд╛рдИ рдЧрдИ рд╣реИред\n",
        "рдЗрд╕ рднрд╛рдЧ рдХрд╛ рдЙрджреНрджреЗрд╢реНрдп рдмрд╣реБрднрд╛рд╖реА рд╕рдорд░реНрдерди рдХреА рдЬрд╛рдВрдЪ рдХрд░рдирд╛ рд╣реИред\n",
        "рднрд╛рд░рдд рдЬреИрд╕реЗ рджреЗрд╢ рдореЗрдВ рдХрдИ рднрд╛рд╖рд╛рдПрдБ рдФрд░ рд╡рд┐рднрд┐рдиреНрди рдЙрдЪреНрдЪрд╛рд░рдг рдкрд╛рдП рдЬрд╛рддреЗ рд╣реИрдВред\n",
        "рдПрдХ рдордЬрдмреВрдд рдПрдПрд╕рдЖрд░ рдкреНрд░рдгрд╛рд▓реА рдХреЛ рд╡рд┐рднрд┐рдиреНрди рднрд╛рд╖рд╛рдУрдВ рдФрд░ рд╢реЛрд░рдпреБрдХреНрдд рд╡рд╛рддрд╛рд╡рд░рдг рдореЗрдВ рднреА рд╕рд╣реА рдкрд╣рдЪрд╛рди рдХрд░рдиреА рдЪрд╛рд╣рд┐рдПред\n",
        "рдЗрд╕ рдСрдбрд┐рдпреЛ рдореЗрдВ рд╕реНрдкрд╖реНрдЯ рдЙрдЪреНрдЪрд╛рд░рдг рдФрд░ рдкреНрд░рд╛рдХреГрддрд┐рдХ рд╡рд╛рдХреНрдп рд╕рдВрд░рдЪрдирд╛ рд╢рд╛рдорд┐рд▓ рд╣реИред\n",
        "рдпрд╣ рдЦрдВрдб рдореЙрдбрд▓ рдХреА рдмрд╣реБрднрд╛рд╖реА рдХреНрд╖рдорддрд╛ рдХрд╛ рдкрд░реАрдХреНрд╖рдг рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛рдПрдЧрд╛ред\n",
        "\"\"\"\n",
        "\n",
        "tts = gTTS(text=hindi_text, lang='hi')\n",
        "tts.save(\"segment5.mp3\")\n",
        "\n",
        "print(\"Hindi TTS segment saved as segment5.mp3\")\n"
      ],
      "metadata": {
        "id": "hypnDnH8-P34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i segment5.mp3 segment5.wav\n"
      ],
      "metadata": {
        "id": "ruYLgKc5-qnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segment 6 тАУ Synthetic English Voice\n",
        "\n",
        "This segment uses generated speech (Text-to-Speech).\n",
        "\n",
        "Purpose:\n",
        "- Compare model performance on artificial vs human speech\n",
        "- Analyze pronunciation clarity effects\n"
      ],
      "metadata": {
        "id": "jHyoBD1TIQ3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "\n",
        "english_tts_text = \"\"\"\n",
        "This is a synthetic voice generated for automatic speech recognition evaluation.\n",
        "Including generated speech allows us to compare human speech with artificial speech patterns.\n",
        "Some ASR systems may perform better on synthetic audio because it has clearer pronunciation.\n",
        "However, real-world human speech often contains natural pauses, emotion, and background noise.\n",
        "This segment is included to test how the recognition model handles artificial speech.\n",
        "\"\"\"\n",
        "\n",
        "tts = gTTS(text=english_tts_text, lang='en')\n",
        "tts.save(\"segment6.mp3\")\n",
        "\n",
        "print(\"English TTS segment saved as segment6.mp3\")\n"
      ],
      "metadata": {
        "id": "BC0NWHEy-yjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segment 7 тАУ Informal Conversational Speech\n",
        "\n",
        "This segment includes natural conversational pacing with filler words and informal tone.\n",
        "\n",
        "Purpose:\n",
        "- Simulate real meeting environments\n",
        "- Evaluate spontaneous speech handling\n"
      ],
      "metadata": {
        "id": "D4frnxdLIfhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "record_audio(\"segment7.wav\", record_seconds=45)\n"
      ],
      "metadata": {
        "id": "uqKNjGfN_pfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i segment6.mp3 segment6.wav\n"
      ],
      "metadata": {
        "id": "oEAtMxMe-8vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Audio Concatenation and Final Formatting\n",
        "\n",
        "All seven segments are combined into a single continuous audio file.\n",
        "\n",
        "The final file:\n",
        "- Exceeds 8 minutes\n",
        "- Is converted to WAV format\n",
        "- Resampled to 16 kHz\n",
        "- Converted to mono\n",
        "\n",
        "This ensures compliance with evaluation requirements.\n"
      ],
      "metadata": {
        "id": "15dJfXGZIlp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -y \\\n",
        "-i segment1.wav \\\n",
        "-i segment2.wav \\\n",
        "-i segment3.wav \\\n",
        "-i segment4.wav \\\n",
        "-i segment5.wav \\\n",
        "-i segment6.wav \\\n",
        "-i segment7.wav \\\n",
        "-filter_complex \"[0:0][1:0][2:0][3:0][4:0][5:0][6:0]concat=n=7:v=0:a=1[out]\" \\\n",
        "-map \"[out]\" combined.wav\n"
      ],
      "metadata": {
        "id": "dURJwclL_Gx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -y -i combined.wav -ar 16000 -ac 1 input_audio.wav\n"
      ],
      "metadata": {
        "id": "UJmDyeRgAHkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Audio Verification\n",
        "\n",
        "This step verifies:\n",
        "\n",
        "- Sampling rate\n",
        "- Duration\n",
        "- Compliance with required 16 kHz mono format\n"
      ],
      "metadata": {
        "id": "D-ktmprYI0gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "\n",
        "y, sr = librosa.load(\"input_audio.wav\", sr=None)\n",
        "\n",
        "print(\"Sample Rate:\", sr)\n",
        "print(\"Duration (minutes):\", len(y)/sr/60)\n"
      ],
      "metadata": {
        "id": "NsbZeZg2_OO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Ground Truth (Reference Transcript)\n",
        "\n",
        "A manual reference transcript was created to match exactly what was spoken.\n",
        "\n",
        "Important:\n",
        "- Repetitions preserved\n",
        "- Multilingual text included\n",
        "- Lowercase normalization applied\n",
        "- Punctuation removed for fair WER calculation\n"
      ],
      "metadata": {
        "id": "tsgvYUds83Om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reference.txt\n",
        "hello this is a test recording for evaluating automatic speech recognition systems today i am speaking in my natural indian english accent artificial intelligence is transforming industries across the world from healthcare to finance we should learn models for improving efficiency however speech recognition systems still face challenges when dealing with different accents and voicing environments in india pronunciation may vary depending on regional background words like data schedule and advertisement may sound slightly different this recording includes natural passes and variations in speaking speed the purpose of this segment is to evaluate how well the asr handles indian accented english in a realistic environment the small passes the purpose of this segment is to evaluate natural passes and variations in speaking speed words like data schedule and advertisement may sound slightly different however speech recognition systems still face challenges when dealing with different accents and voicing environments so this is a test recording for evaluating automatic speech recognition systems today i am speaking in my natural indian english accent artificial intelligence is transforming industries across the world\n",
        "\n",
        "hi everyone this is the second segment of the speech recognition evaluation recording in this part i am speaking in a more american style of english pronunciation automatic speech recognition systems have improved significantly in recent years ai models are now capable of handling different accents background noise and spontaneous speech patterns however performance can still vary depending on pronunciation speaking speed and environmental conditions for example when people speak quite quickly or when there are overlapping sounds in the background recognition accuracy may decrease this section is designed to evaluate how well the asr model adapts to accent variation combined with light background noise hi everyone this is the second segment of the speech recognition evaluation recording in this part i am speaking in a more american style of english pronunciation automatic speech recognition systems have improved significantly in recent years ai models are now capable of handling different accents background noise and spontaneous speech patterns\n",
        "\n",
        "good afternoon this is the third segment of the evaluation recording in this section i am speaking in a british style accent speech recognition systems must be robust across different regions and pronunciation styles in the united kingdom certain words are pronounced differently compared to american english for example words such as advertisement schedule and laboratory may have distinct pronunciation patterns additionally background noise and environmental disturbance can make transcription more challenging this recording includes natural passes and realistic conversational pacing the objective is to test how well the asr system performs under accented speech with noticeable environmental noise good afternoon this is the third segment of the evaluation recording in this section i am speaking in a british style accent speech recognition systems must be robust across different regions and pronunciation styles in the united kingdom certain words are pronounced differently compared to american english for example words such as advertisement schedule and laboratory may have distinct pronunciation patterns additionally background noise and environmental disturbance can make transcription more challenging this recording includes natural passes and realistic conversational pacing the objective is to test how well the asr system performs under accented speech with noticeable environmental noise good afternoon this is\n",
        "\n",
        "роЗройрпНро▒рпБ роиро╛ройрпН родрооро┐ро┤рпН роорпКро┤ро┐ропро┐ро▓рпН рокрпЗроЪрпБроХро┐ро▒рпЗройрпН роЗроирпНрод рокродро┐ро╡рпБ родро╛ройро┐ропроЩрпНроХро┐ роХрпБро░ро▓рпН роЕроЯрпИропро╛ро│ роЕроорпИрокрпНрокрпБроХро│рпИ роородро┐рокрпНрокрпАроЯрпБ роЪрпЖропрпНро╡родро▒рпНроХро╛роХ роЙро░рпБро╡ро╛роХрпНроХрокрпНрокроЯрпБроХро┐ро▒родрпБ рокро▓ роорпКро┤ро┐роХро│рпИ роЪро░ро┐ропро╛роХ роЕроЯрпИропро╛ро│роорпН роХро╛рогрпБроорпН родро┐ро▒ройрпН рооро┐роХро╡рпБроорпН роорпБроХрпНроХро┐ропрооро╛ройродрпБ роЗроирпНродро┐ропро╛ рокрпЛройрпНро▒ роиро╛роЯрпБроХро│ро┐ро▓рпН рокро▓ роорпКро┤ро┐роХро│рпН рооро▒рпНро▒рпБроорпН рокро▓рпНро╡рпЗро▒рпБ роЙроЪрпНроЪро░ро┐рокрпНрокрпБроХро│рпН роХро╛рогрокрпНрокроЯрпБроХро┐ройрпНро▒рой родрооро┐ро┤рпН роорпКро┤ро┐ роТро░рпБ рокро┤роорпИропро╛рой рооро▒рпНро▒рпБроорпН роЪрпЖро┤рпБроорпИропро╛рой роорпКро┤ро┐ропро╛роХрпБроорпН роЙро▓роХроорпН роорпБро┤рпБро╡родрпБроорпН роХрпЛроЯро┐роХрпНроХрогроХрпНроХро╛рой роороХрпНроХро│рпН родрооро┐ро┤рпН рокрпЗроЪрпБроХро┐ройрпНро▒ройро░рпН роХрпБро░ро▓рпН роЕроЯрпИропро╛ро│ роЕроорпИрокрпНрокрпБроХро│рпН ро╡рпЖро╡рпНро╡рпЗро▒рпБ роорпКро┤ро┐роХро│ро┐ро▓рпН роЙро│рпНро│ роТро▓ро┐ро╡роЯро┐ро╡роЩрпНроХро│рпИ рокрпБро░ро┐роирпНродрпБ роХрпКро│рпНро│ ро╡рпЗрогрпНроЯрпБроорпН роХрпБро▒ро┐рокрпНрокро╛роХ роЙропро┐ро░рпЖро┤рпБродрпНродрпБроХро│рпН рооро▒рпНро▒рпБроорпН роорпЖропрпНропрпЖро┤рпБродрпНродрпБроХро│ро┐ройрпН ро╡рпЗро▒рпБрокро╛роЯрпБ родрпЖро│ро┐ро╡ро╛роХ роЕроЯрпИропро╛ро│роорпН роХро╛рогрокрпНрокроЯ ро╡рпЗрогрпНроЯрпБроорпН рокро┐ройрпНройрогро┐ роЪродрпНродроорпН рокрпЗроЪрпБроорпН ро╡рпЗроХ рооро╛ро▒рпНро▒роорпН рооро▒рпНро▒рпБроорпН ро╡ро╛роХрпНроХро┐роп роЗроЯрпИро╡рпЖро│ро┐роХро│рпН роЖроХро┐ропро╡рпИ роХрпБро░ро▓рпН роЕроЯрпИропро╛ро│ роЕроорпИрокрпНрокро┐ройрпН роЪрпЖропро▓рпНродро┐ро▒ройрпИ рокро╛родро┐роХрпНроХро▓ро╛роорпН роЙродро╛ро░рогрооро╛роХ роТро░рпБро╡ро░рпН роорпЖродрпБро╡ро╛роХ рокрпЗроЪрпБроорпНрокрпЛродрпБ рооро▒рпНро▒рпБроорпН ро╡рпЗроХрооро╛роХ рокрпЗроЪрпБроорпНрокрпЛродрпБ роТро▓ро┐ропро┐ройрпН родройрпНроорпИ рооро╛ро▒рпБроорпН роЪро┐ро▓ роирпЗро░роЩрпНроХро│ро┐ро▓рпН роороХрпНроХро│рпН роЙро░рпИропро╛роЯрпБроорпНрокрпЛродрпБ роЗроЯрпИропро┐ро▓рпН роЪро┐ро▒ро┐роп роЗроЯрпИро╡рпЖро│ро┐роХро│рпН ро╡ро┐роЯрпБро╡ро╛ро░рпНроХро│рпН роЕродрпБ роЗропро▓рпНрокро╛рой рокрпЗроЪрпНроЪро┐ройрпН роТро░рпБ рокроХрпБродро┐ропро╛роХрпБроорпН роЗроирпНрод рокроХрпБродро┐ рокро▓ роорпКро┤ро┐ роЖродро░ро╡рпИ роЪрпЛродро┐рокрпНрокродро▒рпНроХро╛роХ роЪрпЗро░рпНроХрпНроХрокрпНрокроЯрпНроЯрпБро│рпНро│родрпБ роЗроирпНрод рооро╛родро┐ро░ро┐ рокродро┐ро╡рпБ роЙрогрпНроорпИропро╛рой роЪрпВро┤рпНроиро┐ро▓рпИропро┐ро▓рпН рокро┐ро░родро┐рокро▓ро┐роХрпНроХро┐ро▒родрпБ\n",
        "\n",
        "рдирдорд╕реНрддреЗ рдпрд╣ рд░рд┐рдХреЙрд░реНрдбрд┐рдВрдЧ рд╣рд┐рдВрджреА рднрд╛рд╖рд╛ рдореЗрдВ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд╡рд╛рдХреН рдкрд╣рдЪрд╛рди рдкреНрд░рдгрд╛рд▓реА рдХреЗ рдореВрд▓реНрдпрд╛рдВрдХрди рдХреЗ рд▓рд┐рдП рдмрдирд╛рдИ рдЧрдИ рд╣реИ рдЗрд╕ рднрд╛рдЧ рдХрд╛ рдЙрджреНрджреЗрд╢реНрдп рдмрд╣реБрднрд╛рд╖реА рд╕рдорд░реНрдерди рдХреА рдЬрд╛рдВрдЪ рдХрд░рдирд╛ рд╣реИ рднрд╛рд░рдд рдЬреИрд╕реЗ рджреЗрд╢ рдореЗрдВ рдХрдИ рднрд╛рд╖рд╛рдПрдВ рдФрд░ рд╡рд┐рднрд┐рдиреНрди рдЙрдЪреНрдЪрд╛рд░рдг рдкрд╛рдП рдЬрд╛рддреЗ рд╣реИрдВ рдПрдХ рдордЬрдмреВрдд рдПрдПрд╕рдЖрд░ рдкреНрд░рдгрд╛рд▓реА рдХреЛ рд╡рд┐рднрд┐рдиреНрди рднрд╛рд╖рд╛рдУрдВ рдФрд░ рд╢реЛрд░рдпреБрдХреНрдд рд╡рд╛рддрд╛рд╡рд░рдг рдореЗрдВ рднреА рд╕рд╣реА рдкрд╣рдЪрд╛рди рдХрд░рдиреА рдЪрд╛рд╣рд┐рдП рдЗрд╕ рдСрдбрд┐рдпреЛ рдореЗрдВ рд╕реНрдкрд╖реНрдЯ рдЙрдЪреНрдЪрд╛рд░рдг рдФрд░ рдкреНрд░рд╛рдХреГрддрд┐рдХ рд╡рд╛рдХреНрдп рд╕рдВрд░рдЪрдирд╛ рд╢рд╛рдорд┐рд▓ рд╣реИ рдпрд╣ рдЦрдВрдб рдореЙрдбрд▓ рдХреА рдмрд╣реБрднрд╛рд╖реА рдХреНрд╖рдорддрд╛ рдХрд╛ рдкрд░реАрдХреНрд╖рдг рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЬрд╛рдПрдЧрд╛\n",
        "\n",
        "this is a synthetic voice generated for automatic speech recognition evaluation including generated speech allows us to compare human speech with artificial speech patterns some asr systems may perform better on synthetic audio because it has clearer pronunciation however real world human speech often contains natural pauses emotion and background noise this segment is included to test how the recognition model handles artificial speech\n",
        "\n",
        "ok so this is an additional short segment added to ensure the total audio duration exceeds eight minutes in real world scenarios people often speak informally and sometimes change topics appropriately there may also be slight hesitations filler words like hmm or you know and catch your pace this helps simulate a more realistic meeting or conversational environment the purpose of this short addition is to satisfy the minimum duration requirement while maintaining natural speech characteristics\n"
      ],
      "metadata": {
        "id": "rG6_2KcmBITT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh reference.txt\n"
      ],
      "metadata": {
        "id": "i2JeVShWBOYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n"
      ],
      "metadata": {
        "id": "cWmGyNICBWHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "OPENAI_BASE_URL = userdata.get(\"OPENAI_BASE_URL\")\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    base_url=OPENAI_BASE_URL\n",
        ")\n"
      ],
      "metadata": {
        "id": "GDt7aRjUBl_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.Whisper-1 Model Transcription\n",
        "\n",
        "Whisper-1 is a transformer-based multilingual ASR model trained on diverse large-scale datasets.\n",
        "\n",
        "Key features:\n",
        "- Multilingual capability\n",
        "- Robustness to accent variation\n",
        "- Noise tolerance\n",
        "- Fast inference speed\n",
        "\n",
        "This section transcribes the complete audio file using Whisper-1.\n",
        "\n"
      ],
      "metadata": {
        "id": "URr9Zn_6G5DC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with open(\"input_audio.wav\", \"rb\") as audio_file:\n",
        "    transcript = client.audio.transcriptions.create(\n",
        "        model=\"whisper-1\",\n",
        "        file=audio_file\n",
        "    )\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "whisper_text = transcript.text\n",
        "\n",
        "with open(\"whisper_prediction.txt\", \"w\") as f:\n",
        "    f.write(whisper_text)\n",
        "\n",
        "whisper_processing_time = end_time - start_time\n",
        "\n",
        "print(\"Whisper transcription saved.\")\n",
        "print(\"Processing time (seconds):\", whisper_processing_time)\n"
      ],
      "metadata": {
        "id": "D81bBRKwBrDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torchaudio jiwer\n"
      ],
      "metadata": {
        "id": "XO4GGcRNCUc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.Wav2Vec2 Model Transcription\n",
        "\n",
        "The second ASR system evaluated is:\n",
        "\n",
        "facebook/wav2vec2-base-960h\n",
        "\n",
        "Model characteristics:\n",
        "- CTC-based architecture\n",
        "- Primarily trained on English speech\n",
        "- Not multilingual\n",
        "- Sensitive to noise and accent variations\n",
        "\n",
        "This model is used to compare multilingual robustness and efficiency.\n"
      ],
      "metadata": {
        "id": "qDaY4L_fG8GU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "speech, sr = librosa.load(\"input_audio.wav\", sr=16000)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "inputs = processor(speech, return_tensors=\"pt\", sampling_rate=16000)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "wav2vec_text = processor.decode(predicted_ids[0])\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "with open(\"other_model_prediction.txt\", \"w\") as f:\n",
        "    f.write(wav2vec_text)\n",
        "\n",
        "wav2vec_processing_time = end_time - start_time\n",
        "\n",
        "print(\"Wav2Vec2 transcription saved.\")\n",
        "print(\"Processing time (seconds):\", wav2vec_processing_time)\n"
      ],
      "metadata": {
        "id": "kosBu73mCWyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jiwer import wer, cer\n",
        "\n",
        "with open(\"reference.txt\") as f:\n",
        "    reference = f.read()\n",
        "\n",
        "with open(\"whisper_prediction.txt\") as f:\n",
        "    whisper_pred = f.read()\n",
        "\n",
        "with open(\"other_model_prediction.txt\") as f:\n",
        "    other_pred = f.read()\n"
      ],
      "metadata": {
        "id": "1U6uMj7cCZZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.Evaluation Metrics\n",
        "\n",
        "## Word Error Rate (WER)\n",
        "WER = (Substitutions + Insertions + Deletions) / Total Words\n",
        "\n",
        "Measures word-level transcription accuracy.\n",
        "\n",
        "## Character Error Rate (CER)\n",
        "Measures character-level transcription errors.\n",
        "\n",
        "## Real-Time Factor (RTF)\n",
        "RTF = Processing Time / Audio Duration\n",
        "\n",
        "- RTF < 1 тЖТ Faster than real-time\n",
        "- RTF > 1 тЖТ Slower than real-time\n",
        "\n",
        "## Inverse RTF\n",
        "Indicates how many times faster than real-time the model operates.\n"
      ],
      "metadata": {
        "id": "WNZq3lplG_kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_wer = wer(reference, whisper_pred)\n",
        "whisper_cer = cer(reference, whisper_pred)\n",
        "\n",
        "other_wer = wer(reference, other_pred)\n",
        "other_cer = cer(reference, other_pred)\n",
        "\n",
        "print(\"Whisper WER:\", whisper_wer)\n",
        "print(\"Whisper CER:\", whisper_cer)\n",
        "\n",
        "print(\"Wav2Vec2 WER:\", other_wer)\n",
        "print(\"Wav2Vec2 CER:\", other_cer)\n"
      ],
      "metadata": {
        "id": "XKz2nbQSCagH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "\n",
        "y, sr = librosa.load(\"input_audio.wav\", sr=None)\n",
        "audio_duration = len(y) / sr\n",
        "\n",
        "whisper_rtf = whisper_processing_time / audio_duration\n",
        "wav2vec_rtf = wav2vec_processing_time / audio_duration\n",
        "\n",
        "whisper_inverse_rtf = 1 / whisper_rtf\n",
        "wav2vec_inverse_rtf = 1 / wav2vec_rtf\n",
        "\n",
        "print(\"Whisper RTF:\", whisper_rtf)\n",
        "print(\"Whisper Inverse RTF:\", whisper_inverse_rtf)\n",
        "\n",
        "print(\"Wav2Vec2 RTF:\", wav2vec_rtf)\n",
        "print(\"Wav2Vec2 Inverse RTF:\", wav2vec_inverse_rtf)\n"
      ],
      "metadata": {
        "id": "5eRJQKveCbsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"results.txt\", \"w\") as f:\n",
        "    f.write(\"WHISPER RESULTS\\n\")\n",
        "    f.write(f\"WER: {whisper_wer}\\n\")\n",
        "    f.write(f\"CER: {whisper_cer}\\n\")\n",
        "    f.write(f\"RTF: {whisper_rtf}\\n\")\n",
        "    f.write(f\"Inverse RTF: {whisper_inverse_rtf}\\n\\n\")\n",
        "\n",
        "    f.write(\"WAV2VEC2 RESULTS\\n\")\n",
        "    f.write(f\"WER: {other_wer}\\n\")\n",
        "    f.write(f\"CER: {other_cer}\\n\")\n",
        "    f.write(f\"RTF: {wav2vec_rtf}\\n\")\n",
        "    f.write(f\"Inverse RTF: {wav2vec_inverse_rtf}\\n\")\n",
        "\n",
        "print(\"results.txt created successfully.\")\n"
      ],
      "metadata": {
        "id": "l_FqBM55CdKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.Model Comparison\n",
        "\n",
        "| Metric | Whisper | Wav2Vec2 |\n",
        "|--------|----------|------------|\n",
        "| WER | 0.49 | 1.22 |\n",
        "| CER | 0.27 | 0.87 |\n",
        "| RTF | 0.031 | 1.43 |\n",
        "\n",
        "Observations:\n",
        "\n",
        "- Whisper significantly outperformed Wav2Vec2 in multilingual scenarios.\n",
        "- Wav2Vec2 struggled with Tamil and Hindi speech.\n",
        "- Whisper demonstrated better robustness to accent and noise.\n",
        "- Whisper achieved real-time capable performance.\n"
      ],
      "metadata": {
        "id": "xsdKn2YwJexO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.Results Summary\n",
        "\n",
        "## Whisper-1\n",
        "- WER: 0.49\n",
        "- CER: 0.27\n",
        "- RTF: 0.031\n",
        "- 31├Ч faster than real-time\n",
        "\n",
        "## Wav2Vec2\n",
        "- WER: 1.22\n",
        "- CER: 0.87\n",
        "- RTF: 1.43\n",
        "- Slower than real-time\n"
      ],
      "metadata": {
        "id": "YEzwu3ZJHCaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.Final Observations\n",
        "\n",
        "1. Multilingual transformer-based ASR models outperform monolingual CTC-based models in real-world conditions.\n",
        "\n",
        "2. Whisper handled:\n",
        "   - Accent variations\n",
        "   - Background noise\n",
        "   - Multilingual speech\n",
        "   more effectively than Wav2Vec2.\n",
        "\n",
        "3. Wav2Vec2 failed significantly in non-English segments.\n",
        "\n",
        "4. Whisper achieved real-time processing capability (RTF < 1).\n",
        "\n",
        "5. Lower CER compared to WER indicates many errors were minor word mismatches rather than completely incorrect predictions.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Whisper-1 is better suited for real-world multilingual ASR applications compared to Wav2Vec2.\n"
      ],
      "metadata": {
        "id": "P86SPCU4HFDw"
      }
    }
  ]
}